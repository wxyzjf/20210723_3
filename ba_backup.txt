[mapr@bigdatamr10 ba]$ cat row.py
from pyspark.sql import Row

myRow = Row("Hello",None,1,False)
print(myRow.printSchema())

-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat row.py
from pyspark.sql import Row

myRow = Row("Hello",None,1,False)
print(myRow.printSchema())
[mapr@bigdatamr10 ba]$ cat test.py
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

"""
A simple example demonstrating basic Spark SQL features.
Run with:
  ./bin/spark-submit examples/src/main/python/sql/basic.py
"""
from __future__ import print_function

# $example on:init_session$
from pyspark.sql import SparkSession
# $example off:init_session$

# $example on:schema_inferring$
from pyspark.sql import  Row
# $example off:schema_inferring$

# $example on:programmatic_schema$
# Import data types
from pyspark.sql.types import *
# $example off:programmatic_schema$


def basic_df_example(spark):
    # $example on:create_df$
    # spark is an existing SparkSession
    df = spark.read.csv("/HDS_VOL_HIVE/miep_domain/domains.csv")
    # Displays the content of the DataFrame to stdout
    df.printSchema()
    df.show()
    df.createOrReplaceTempView("t_table")
    data=spark.sql("select _c0,count(1),sum(_c1) from t_table group by _c0")
    data.show(10)
    data.write.format("file:///home/mapr/ba/domain.csv")
    
    # +----+-------+
    # | age|   name|
    # +----+-------+
    # |null|Michael|
    # |  30|   Andy|
    # |  19| Justin|
    # +----+-------+
    # $example off:create_df$
    

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()


-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat test4.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime

def basic_df_example(spark):

    myDomainSchema = StructType([ StructField("domain", StringType(), True),
                              StructField("media_category_name", StringType(), True),
                              StructField("media_sub_category_name", IntegerType(), True),
                              StructField("media_provider_name", IntegerType(), True)
                            ])

 

    myMiepSchema = StructType([ StructField("subr_num", StringType(), True),
                              StructField("subr_url", StringType(), True),
                              StructField("uload_size", IntegerType(), True),
                              StructField("dnld_size", IntegerType(), True),
                              StructField("call_dur", IntegerType(), True),
                              StructField("charging_id", StringType(), True),
                              StructField("accs_type_cd", StringType(), True),
                              StructField("accs_point_name", StringType(), True),
                              StructField("sgsn_ip_addr", StringType(), True),
                              StructField("radio_accs_type_cd", StringType(), True),
                              StructField("src_ip_addr", StringType(), True),
                              StructField("imsi", StringType(), True),
                              StructField("accs_date", StringType(), True),
                              StructField("accs_time", StringType(), True),
                              StructField("status", StringType(), True),
                              StructField("user_agent", StringType(), True),
                              StructField("statuscode", StringType(), True),
                              StructField("imei", StringType(), True),
                              StructField("dialleddigit", StringType(), True),
                              StructField("domain", StringType(), True),
                              StructField("content_type", StringType(), True),
                              StructField("part_key", IntegerType(), True),
                            ])

    miep = spark.read.schema(myMiepSchema).csv("/HDS_VOL_HIVE/miep/*.gz",sep='\t')
    domain = spark.read.schema(myDomainSchema).csv("/HDS_VOL_HIVE/miep_domain/domains.csv")
    
    miep.createOrReplaceTempView("miep")
    domain.createOrReplaceTempView("domain")
    data=spark.sql("select m.subr_num,\
                           m.accs_date,\
                           d.media_category_name,\
                           d.media_sub_category_name,\
                           count(1), \
                           sum(m.uload_size)+sum(m.dnld_size)\
                      from miep m \
                      left outer join domain d on m.domain = d.domain \
                     group by m.subr_num,m.accs_date,d.media_category_name,d.media_sub_category_name")
    data.write.save("maprfs:///HDS_VOL_TMP/test_miep_ba",format='csv',mode='append')
    data.show(20)

spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()
print(datetime.now())
basic_df_example(spark)
print(datetime.now())

-------------------------------------------------------------------------------------

[mapr@bigdatamr10 ba]$ cat test5.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime

def basic_df_example(spark):

    myDomainSchema = StructType([ StructField("domain", StringType(), True),
                              StructField("media_category_name", StringType(), True),
                              StructField("media_sub_category_name", IntegerType(), True),
                              StructField("media_provider_name", IntegerType(), True)
                            ])

 

    myMiepSchema = StructType([ StructField("subr_num", StringType(), True),
                              StructField("subr_url", StringType(), True),
                              StructField("uload_size", IntegerType(), True),
                              StructField("dnld_size", IntegerType(), True),
                              StructField("call_dur", IntegerType(), True),
                              StructField("charging_id", StringType(), True),
                              StructField("accs_type_cd", StringType(), True),
                              StructField("accs_point_name", StringType(), True),
                              StructField("sgsn_ip_addr", StringType(), True),
                              StructField("radio_accs_type_cd", StringType(), True),
                              StructField("src_ip_addr", StringType(), True),
                              StructField("imsi", StringType(), True),
                              StructField("accs_date", StringType(), True),
                              StructField("accs_time", StringType(), True),
                              StructField("status", StringType(), True),
                              StructField("user_agent", StringType(), True),
                              StructField("statuscode", StringType(), True),
                              StructField("imei", StringType(), True),
                              StructField("dialleddigit", StringType(), True),
                              StructField("domain", StringType(), True),
                              StructField("content_type", StringType(), True),
                              StructField("part_key", IntegerType(), True),
                            ])

    miep = spark.read.schema(myMiepSchema).csv("/HDS_VOL_HIVE/miep/SMCIN_*gz",sep='\t')
    domain = spark.read.schema(myDomainSchema).csv("/HDS_VOL_HIVE/miep_domain/domains.csv")
    
    miep.createOrReplaceTempView("miep")
    domain.createOrReplaceTempView("domain")
    
    #spark.catalog.cacheTable("domain")    

    spark.sql("""select subr_num,
                        accs_date,
                        domain,
                        sum(uload_size+dnld_size)as sum_vol,
                        count(*) cnt 
                   from miep 
                  group by subr_num,accs_date,domain""").createOrReplaceTempView("miepgrp")
    
    spark.sql("""select m.subr_num,
                        m.accs_date,
                        m.domain,
                        d.media_category_name,
                        d.media_sub_category_name,
                        sum(sum_vol),
                        sum(cnt)
                   from miepgrp m
                   left join domain d on m.domain = d.domain
                  group by m.subr_num,m.accs_date,m.domain,d.media_category_name,d.media_sub_category_name 
                """).write.save("maprfs:///HDS_VOL_TMP/test_miep_ba",format='csv',mode='overwrite')




spark = SparkSession \
    .builder \
    .appName("test5") \
    .config("spark.some.config.option", "some-value") \
    .config("spark.executor.instances","100")\
    .config("spark.executor.memory","6G")\
    .config("spark.executor.cores","4")\
    .getOrCreate()
print(datetime.now())
basic_df_example(spark)
print(datetime.now())

-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat test6.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime

def basic_df_example(spark):

    result = spark.read.csv("/HDS_VOL_TMP/test_miep_ba/*.csv")
    result.createOrReplaceTempView("result")
     
    spark.sql("select count(1) from result").show()

spark = SparkSession \
    .builder \
    .appName("test5") \
    .config("spark.some.config.option", "some-value") \
    .config("spark.executor.instances","100")\
    .config("spark.executor.memory","6G")\
    .config("spark.executor.cores","4")\
    .getOrCreate()
print(datetime.now())
basic_df_example(spark)
print(datetime.now())
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat test2.py
import smtutil import hiveutil

sc, spark = hiveutil.get_sc_spark();

line = "010.247.114.103"
line[8:14].replace(".", "").show()

-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat parque.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark import SparkContext,SparkConf

from pyspark.sql import  Row

from pyspark.sql.types import *

from pyspark.sql.functions import *

from datetime import datetime

import math

    
conf = SparkConf().setAppName("Test App").setAll([('spark.executor.instances',13),('soark.executor.memory','3G'),('spark.executor.memoryOverhead','3G')])
sc = SparkContext(conf=conf)

def partitionF(key):
    return key

print(datetime.now())
rdd = sc.textFile("/HDS_VOL_HIVE/FWCDR/start_date=20200627/*gz",use_unicode=False)
fwcdr = rdd.map(lambda line : (int(line[8:11]),line))\
        .repartitionAndSortWithinPartitions(255,lambda k : k%255, True)
print(fwcdr.getNumPartitions())

spark = SparkSession(sc)
df = spark.createDataFrame(fwcdr)

df.write.save("maprfs:///HDS_VOL_TMP/test_par_ba",mode='overwrite',compression='gzip')
print(datetime.now())
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat parque3.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime

def basic_df_example(spark):
    
    mySchema = StructType([
                StructField("rowkey",StringType(), True),
                StructField("ts",StringType(), True),
                StructField("srcip",StringType(), True),
                StructField("tagip",StringType(), True),
                StructField("col5",StringType(), True)
                ])
    fwcdr = spark.read.schema(mySchema).csv("/HDS_VOL_HIVE/FWCDR/start_date=20200703/*.gz",sep='|')
    fwcdr.createOrReplaceTempView("fwcdr")
    spark.sql("select substr(rowkey,9,2) as partitionKey,count(1) from fwcdr group by substr(rowkey,9,2)").show(26)
    


spark = SparkSession \
    .builder \
    .appName("parque test") \
    .config("spark.executor.memory","4G")\
    .config('spark.executor.memoryOverhead','5G')\
    .config('spark.executor.instances',12)\
    .getOrCreate()
print(datetime.now())
basic_df_example(spark)
print(datetime.now())
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat ptest.py
x = ['a','a','b','b','v','a','a','a','c','c','a','a','a']

a = 0
while a < len(x)-1:
  print ('a:',a,'len:',len(x)-1)
  if x[a] == x[a+1]:
     print('remove:',x[a+1])
     del x[a+1]
     print('x[0]:',x[0])
  else:
    a += 1
print(x)
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat ptest.py
x = ['a','a','b','b','v','a','a','a','c','c','a','a','a']

a = 0
while a < len(x)-1:
  print ('a:',a,'len:',len(x)-1)
  if x[a] == x[a+1]:
     print('remove:',x[a+1])
     del x[a+1]
     print('x[0]:',x[0])
  else:
    a += 1
print(x)
[mapr@bigdatamr10 ba]$ cat maprTest.scala

import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.execution.datasources.hbase.{HBaseRelation, HBaseTableCatalog}


def catalog = s"""{
       |"table":{"namespace":"default", "name":"maprfs:///HDS_VOL_TMP/test_mall_ba/mall_ba"},
       |"rowkey":"key",
       |"columns":{
         |"col0":{"cf":"rowkey", "col":"key", "type":"string"},
         |"col1":{"cf":"cf1", "col":"col1", "type":"boolean"},
         |"col2":{"cf":"cf2", "col":"col2", "type":"double"},
         |"col3":{"cf":"cf3", "col":"col3", "type":"float"},
         |"col4":{"cf":"cf4", "col":"col4", "type":"int"},
         |"col5":{"cf":"cf5", "col":"col5", "type":"bigint"},
         |"col6":{"cf":"cf6", "col":"col6", "type":"smallint"},
         |"col7":{"cf":"cf7", "col":"col7", "type":"string"},
         |"col8":{"cf":"cf8", "col":"col8", "type":"tinyint"}
       |}
     |}""".stripMargin

case class HBaseRecord(
   col0: String,
   col1: Boolean,
   col2: Double,
   col3: Float,
   col4: Int,       
   col5: Long,
   col6: Short,
   col7: String,
   col8: Byte)


object HBaseRecord
{                                                                                                             
   def apply(i: Int, t: String): HBaseRecord = {
      val s = s"""row${"%03d".format(i)}"""       
      HBaseRecord(s,
      i % 2 == 0,
      i.toDouble,
      i.toFloat,  
      i,
      i.toLong,
      i.toShort,  
      s"String$i: $t",      
      i.toByte)
  }
}

def withCatalog(cat: String): DataFrame = {
  sqlContext
  .read
  .options(Map(HBaseTableCatalog.tableCatalog->cat))
  .format("org.apache.hadoop.hbase.spark")
  .load()
}

def main(args: Array[String]) {
  val data = (0 to 255).map { i =>  HBaseRecord(i, "extra")}
  val sc = spark.sparkContext
  sc.parallelize(data).toDF.write.options(Map(
        HBaseTableCatalog.tableCatalog -> catalog,
        HBaseTableCatalog.newTable -> "5")
  ).format("org.apache.hadoop.hbase.spark")
   .save()

  val df = withCatalog(catalog)
}

-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat maprTest.java
import org.apache.spark.sql.Row;
import com.mapr.db.spark.sql.api.java.MapRDBJavaSession;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;


public class maprTest {

    public static class myEntity implements Serializable {
        private String time;
        private String eci;
        private String imsi;
        private String timeout;
        private String eciout;

        public void setTime(String time) {
            this.time = time;
        }
        public void setEci(String eci) {
            this.eci = eci;
        }
        public void setImsi(String imsi) {
            this.imsi = imsi;
        }
        public void setTimeout(String timeout) {
            this.timeout = timeout;
        }
        public void setEciout(String eciout) {
            this.eciout = eciout;
        }
    }


    public static void main(String[] args) {
        MapRDBJavaSession maprSession = new MapRDBJavaSession(spark);
        Dataset<myEntity> ds = maprSession.loadFromMapRDB("/HDS_VOL_TMP/test_mall_ba/newTable");
        ds.show();
    }

}

-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat maprTest.py
from pyspark.sql import SparkSession
from pyspark import SparkContext,SparkConf

conf = SparkConf().setAppName("spark connection maprdb test").setMaster('local[5]')
sc = SparkContext(conf=conf)

spark = SparkSession \
    .builder \
    .getOrCreate()

#spark.loadFromMapRDB("maprfs:///HDS_VOL_TMP/test_mall_ba/newTable").show()


df = sc.parallelize([ { "_id": "454065216321941", "eci":"123456", "eciout": "123457", "time": "123456780", "timeout":"123456789"}]).toDF().orderBy("_id")

spark.insertToMapRDB(df,"maprfs:///HDS_VOL_TMP/test_mall_ba/newTable",bulk_insert=True)

spark.loadFromMapRDB("maprfs:///HDS_VOL_TMP/test_mall_ba/newTable").show()
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat parquet2.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime

def basic_df_example(spark):
    
    mySchema = StructType([
                StructField("rowkey",StringType(), True),
                StructField("ts",StringType(), True),
                StructField("srcip",StringType(), True),
                StructField("tagip",StringType(), True),
                StructField("vol",StringType(), True)
                ])
    fwcdr = spark.read.schema(mySchema).csv("/HDS_VOL_HIVE/FWCDR/start_date=20200711/fwcdr_ldr_*_p1_*.gz",sep='|')
    fwcdr.createOrReplaceTempView("fwcdr")    df = spark.sql("select int(substr(rowkey,9,3)/26) as pkey,ts,srcip,tagip,vol from fwcdr")\
         .repartition(25,"pkey")\
         .sortWithinPartitions("pkey","srcip").show(1)


    df.repartition(1).write.save("maprfs:///HDS_VOL_TMP/test_par_ba/test_combine",mode='append',compression='gzip')

spark = SparkSession \
    .builder \
    .appName("parque test") \
    .config("spark.executor.memory","13G")\
    .config('spark.driver.maxResultSize','4G')\
    .config('spark.default.parallelism',15) \
    .config('spark.executor.instances',25) \
    .config('spark.driver.memory','4G') \
    .getOrCreate()
print(datetime.now())
basic_df_example(spark)
print(datetime.now())
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat mall.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime



def sortKey(elem):
    return elem[0]

def sortAndDedupliaction(x):
    x = list(x)
    x.sort(key=sortKey)
    
    a = 0
    while a < len(x)-1:
      if x[a][1] == x[a+1][1]:
        del x[a+1]
      else:
        x[a] = Row(time=x[a][0], eci=x[a][1], imsi=x[a][2], timeout=x[a+1][0], eciout=x[a+1][1])
        a += 1
    x[a] = Row(time=x[a][0], eci=x[a][1], imsi=x[a][2], timeout='0', eciout='')
    return x

def basic_df_example(spark):
    
    fwcdr = spark.read.csv("/HDS_VOL_HIVE/mall_ebm/{1,3,5}.tar.gz",sep=';',header='True')
    fwcdr.createOrReplaceTempView("mallcdr")
    
    rdd = spark.sql("select time,eci,imsi,'0' as timeout, '' as eciout from mallcdr where imsi = '454065200005302' order by time").show(100,False)
    """ 
    sample = rdd.map(lambda row : (row[2],list(row)))\
       .toDF().select('_2').show(10)
       #.write.save("maprfs:///HDS_VOL_TMP/test_mall_ba",compression='gzip',mode='append')
    """
spark = SparkSession \
    .builder \
    .appName("mall test ba") \
    .config("spark.executor.memory","10G")\
    .config('spark.executor.memoryOverhead','5G')\
    .config('spark.executor.instances',15) \
    .config('spark.debug.maxToStringFields',50)\
    .getOrCreate()
print(datetime.now())
basic_df_example(spark)
print(datetime.now())
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat mall2.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime

from pyspark.sql.functions import *



spark = SparkSession \
    .builder \
    .appName("mall test ba") \
    .config("spark.executor.memory","10G")\
    .config('spark.executor.memoryOverhead','5G')\
    .config('spark.executor.instances',15) \
    .config('spark.debug.maxToStringFields',50)\
    .getOrCreate()



df= spark.read.csv("/HDS_VOL_HIVE/mall_ebm/{1,3,4,5}.tar.gz",sep=';',header='True')

#park.read.csv("file:///home/mapr/ka/test.csv",sep=';',header='True')

df.createOrReplaceTempView("mallcdr")

decodeSchema = ArrayType(
    StructType([
        StructField("time", StringType(), False),
        StructField("eci", StringType(), False),
        StructField("timeout", StringType(), False),
        StructField("dwell_time", StringType(), False)
]))


def udf_d(sorted_list):
     
    a = 0
    time_out = 0
    while a < len(sorted_list)-1:
      
      elem = sorted_list[a].split('_',1)
      next_elem = sorted_list[a+1].split('_',1)

      if elem[1] == next_elem[1]:
        time_out = next_elem[0]
        del sorted_list[a+1]
      else:
        if time_out != 0 :
          sorted_list[a] = tuple(elem) + (time_out, float(time_out) - float(elem[0]))
        else:
          sorted_list[a] = tuple(elem) + (elem[0],0)
        time_out = 0
        a += 1

    elem = sorted_list[a].split('_',1)
    if time_out != 0:
      sorted_list[a] = tuple(elem) + (time_out, float(time_out) - float(elem[0]))
    else: 
      sorted_list[a] = tuple(elem) + (elem[0],0)
    return sorted_list



m_udf=udf(lambda s:udf_d(s), decodeSchema)

df = spark.sql("select time,eci,imsi,time||'_'||eci as str from mallcdr") \
        .groupBy('imsi') \
        .agg(m_udf(sort_array(collect_set("str"),True)).alias("eciList")) \
        .rdd.flatMapValues(lambda x : x) \
        .coalesce(10) \
        .toDF() \
        .select('_1','_2.*') \
        .write.save("maprfs:///HDS_VOL_TMP/test_mall_ba",format='csv',mode='append')
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat splitFile.py


from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("slipt file") \
    .config("spark.executor.memory","10G")\
    .config('spark.executor.memoryOverhead','5G')\
    .config('spark.executor.instances',15) \
    .config('spark.debug.maxToStringFields',50)\
    .getOrCreate()

df= spark.read.csv("/HDS_VOL_HIVE/mall_ebm/4.tar.gz",sep=';',header='True') \
        .repartition(10) \
        .write.save("maprfs:///HDS_VOL_HIVE/mall_ebm/split_files",format='csv',compression='gzip',mode='append')
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat querypqTest.py
from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime

def basic_df_example(spark):
    
    mySchema = StructType([
                StructField("pkey",StringType(), True),
                StructField("rowkey",StringType(), True),
                StructField("ts",StringType(), True),
                StructField("srcip",StringType(), True),
                StructField("tagip",StringType(), True),
                StructField("col5",StringType(), True)
                ])
    fwcdr = spark.read.schema(mySchema).parquet("/HDS_VOL_TMP/test_par_ba/*.parquet")
    fwcdr.createOrReplaceTempView("fwcdr")
    spark.sql("select min(pkey) from fwcdr").show()


spark = SparkSession \
    .builder \
    .appName("parque test") \
    .config("spark.executor.memory","10G")\
    .config('spark.executor.memoryOverhead','5G')\
    .config('spark.executor.instances',10)\
    .getOrCreate()
print(datetime.now())
basic_df_example(spark)
print(datetime.now())
-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat genParquetFromLoacl.py


from __future__ import print_function

from pyspark.sql import SparkSession

from pyspark.sql import  Row

from pyspark.sql.types import *

from datetime import datetime




spark = SparkSession \
    .builder \
    .appName("generate parquet file") \
    .config("spark.executor.memory","3G")\
    .config('spark.executor.memoryOverhead','1G')\
    .config('spark.executor.instances',1)\
    .getOrCreate()


df = spark.read.csv('file:///home/mapr/ba/part0000.csv',sep=',')
df.createOrReplaceTempView('fwcdr')

spark.sql('select int(_c0),_c1,_c2 from fwcdr limit 10').\
        write.save('file:///home/mapr/ba/parquet-sample')

-------------------------------------------------------------------------------------

[mapr@bigdatamr10 ba]$ cat joinTest.py
from pyspark.sql import SparkSession
from pyspark.sql import  Row
from pyspark.sql.types import *
from datetime import datetime


spark = SparkSession \
    .builder \
    .appName("mall lqs") \
    .config("spark.executor.memory","10G")\
    .config('spark.executor.memoryOverhead','5G')\
    .config('spark.executor.instances',15) \
    .config('spark.debug.maxToStringFields',50)\
    .getOrCreate()


sch_pgw =StructType([ StructField("subr_num", StringType(), True),
                      StructField("dd_tag", StringType(), True),
                      StructField("cgi", StringType(), True),
                      StructField("landmark_eng", StringType(), True),
                      StructField("landmark_chi", StringType(), True),
                      StructField("ttl_vol", DecimalType(), True)
                    ])

df= spark.read.csv("maprfs:///HDS_VOL_TMP/ka/res/part-*-dbde761a-2aca-4507-8143-d6f964a859da-c000.csv.gz", sep=',', schema=sch_pgw)
df.createOrReplaceTempView("v_pgw")

sch_subr =StructType([ StructField("subr_num", StringType(), True) ])

df2= spark.read.csv("maprfs:///HDS_VOL_TMP/ba/bp_subr_num.txt", sep=',',schema=sch_subr).cache()
df2.createOrReplaceTempView("bp_subr_info")


spark.sql("select v.subr_num, v.dd_tag, v.ttl_vol, v.cgi, v.landmark_eng, v.landmark_chi \
             from v_pgw v \
             left outer join bp_subr_info s \
                  on v.subr_num = s.subr_num \
            where s.subr_num is not null ") \
     .repartition(1)\
     .write.save("maprfs:///HDS_VOL_TMP/ba/res", compression='gzip', format='csv',mode='append')




-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ cat smcinJoin.py
from pyspark.sql import SparkSession
from pyspark.sql import  Row
from pyspark.sql.types import *
from datetime import datetime


spark = SparkSession \
    .builder \
    .appName("mall lqs") \
    .config("spark.executor.memory","8G")\
    .config('spark.executor.memoryOverhead','4G')\
    .config('spark.driver.memory','4G')\
    .config('spark.executor.instances',15) \
    .config('spark.debug.maxToStringFields',50)\
    .getOrCreate()


df= spark.read.csv("maprfs:///HDS_VOL_TMP/ba/unload_bp_s_reward_subr_list_SHK_Mall.dat.20200720",header=True, sep='|').cache()
df.createOrReplaceTempView("subr_info")


df2 = spark.read.csv("maprfs:///HDS_VOL_TMP/smcin/part_key=20200720/SMCIN*.gz", sep='\t')
df2.createOrReplaceTempView("smcin")



spark.sql("select /*+ BORADCAST(subr) */ smc.* from subr_info subr inner join smcin smc on subr.Subr_Num = smc._c0")\
        .repartition(1) \
        .write.save("maprfs:///HDS_VOL_TMP/ba/smcin", compression='gzip', format='csv',mode='append')


-------------------------------------------------------------------------------------
[mapr@bigdatamr10 ba]$ catsmcinsplit.py 
-bash: catsmcinsplit.py: command not found
[mapr@bigdatamr10 ba]$ cat smcinsplit.py

from pyspark.sql import SparkSession
from pyspark.sql import  Row
from pyspark.sql.types import *
from datetime import datetime


spark = SparkSession \
    .builder \
    .appName("smcin file split") \
    .config("spark.executor.memory","10G")\
    .config('spark.executor.memoryOverhead','5G')\
    .config('spark.executor.instances',15) \
    .config('spark.debug.maxToStringFields',50)\
    .getOrCreate()



df2 = spark.read.csv("maprfs:///HDS_VOL_TMP/smcin/part_key=20200720/SMCIN_20200720_20200720145019_01_proc_1.gz", sep='\t')\
        .repartition(3) \
        .write.save("maprfs:///HDS_VOL_TMP/smcin_split", compression='gzip', format='csv',mode='append')
-------------------------------------------------------------------------------------
Jul 23
-------------------------------------------------------------------------------------






































